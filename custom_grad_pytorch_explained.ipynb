{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $A \\in \\mathbb{R}^{m \\times n}, x \\in \\mathbb{R}^{n}$.\n",
    "\n",
    "Let $f(A, x)$ be a simple matrix-vector product $Ax$. \n",
    "\n",
    "And suppose $h$ is some function of $f(A, x)$. It can be multidimensional, but without loss of generality we consider one-dimensional case, e.g. $h(\\cdot) = \\| \\cdot \\|^2$. Vector and matrix functions can be differenciated in the same way.\n",
    "\n",
    "Chain-rule states that\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial x_{i}} g(f(A, x))\n",
    "    =\n",
    "\\sum\\limits_{k}\n",
    "    \\underbrace{\\dfrac{\\partial g(Ax)}{\\partial (Ax)_k}}_\\text{grad_output[k]}\n",
    "\\cdot\n",
    "    \\underbrace{\\dfrac{\\partial (Ax)_k}{\\partial x_i}}_{\\nabla_x (Ax)[k, i]}\n",
    "$$\n",
    "\n",
    "In PyTorch we just have to implement these computations.\n",
    "\n",
    "Gradients w.r.t. A are computed in the same way:\n",
    "$$\n",
    "\\dfrac{\\partial}{\\partial a_{ij}} g(f(A, x))\n",
    "    =\n",
    "\\sum\\limits_{k}\n",
    "    \\underbrace{\\dfrac{\\partial g(Ax)}{\\partial (Ax)_k}}_\\text{$g_k :=$ grad_output[k]}\n",
    "\\cdot\n",
    "    \\underbrace{\\dfrac{\\partial (Ax)_k}{\\partial a_{i, j}}}_{f_{i,j,k} := \\nabla_A (Ax)[i, j, k]}\n",
    "$$\n",
    "\n",
    "It is easy to understand that $f_{i, j, k} = x_j \\cdot [k = i]$, where $[\\cdot]$ is an indicator function. That's why\n",
    "$$\n",
    "\\sum\\limits_{k} g_k f_{i, j, k} = g_i^\\top x_j.\n",
    "$$\n",
    "\n",
    "PyTorch code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_MatVec(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, x):\n",
    "        '''\n",
    "        A: (m, n)\n",
    "        x: (n, )\n",
    "        '''\n",
    "        ctx.save_for_backward(A, x)\n",
    "        return A.mv(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        '''\n",
    "        grad_output: (n, )\n",
    "        '''\n",
    "        A, x = ctx.saved_variables\n",
    "        grad_x = A.t().mv(grad_output)\n",
    "        grad_A = grad_output.unsqueeze(1) * x.unsqueeze(0)\n",
    "        return grad_A, grad_x\n",
    "    \n",
    "matvec = My_MatVec.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 3, 5\n",
    "np_A = np.random.rand(m, n)\n",
    "np_x = np.random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1.9169  1.4665  0.5486  0.4641  1.5120\n",
       "  2.4609  1.8826  0.7043  0.5957  1.9411\n",
       "  2.5767  1.9713  0.7374  0.6238  2.0324\n",
       " [torch.DoubleTensor of size 3x5], Variable containing:\n",
       "  8.0665\n",
       "  3.8004\n",
       "  3.4847\n",
       "  5.7956\n",
       "  4.4506\n",
       " [torch.DoubleTensor of size 5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Variable(torch.from_numpy(np_A), requires_grad=True)\n",
    "x = Variable(torch.from_numpy(np_x), requires_grad=True)\n",
    "res = torch.norm(matvec(A, x)).pow(2)\n",
    "res.backward()\n",
    "A.grad, x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "A = Variable(torch.randn(5, 7).double(), requires_grad=True)\n",
    "x = Variable(torch.randn(7).double(), requires_grad=True)\n",
    "input = (A, x)\n",
    "test = gradcheck(matvec, input, eps=1e-10, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
